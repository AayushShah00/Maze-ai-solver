ðŸ“‘ ResNet-18 Training on Filtered CIFAR-10 DatasetThis document details a complete PyTorch implementation for training a ResNet-18 Convolutional Neural Network (CNN) on a modified version of the CIFAR-10 dataset, featuring 8 classes (referred to here as CIFAR- 8).The code includes the full model definition, advanced data preprocessing with augmentation techniques, hyperparameter optimization typical for high-performance image classification, and comprehensive evaluation.ðŸš€ Key FeaturesResNet-18 Architecture: Implementation of the standard ResNet-18 using PyTorch's nn.Module, built upon the BasicBlock structure featuring residual/skip connections.CIFAR-8 Data Filtering: Custom logic to download the 10-class CIFAR-10 dataset and filter it down to 8 specific classes (plane, car, bird, cat, deer, dog, frog, horse), remapping their labels for the new classification task.Advanced Data Augmentation: Utilizes RandomCrop and RandomHorizontalFlip on the training set, alongside standard normalization, for improved model robustness and generalization.Best Practices Training: Employs Stochastic Gradient Descent (SGD) with Momentum (0.9), L2 Weight Decay ($5 \times 10^{-4}$), and a Cosine Annealing Learning Rate Scheduler over 100 epochs for optimal convergence.GPU Acceleration: Automatic detection and utilization of CUDA for fast training when available.Full Evaluation: Outputs final test accuracy, a detailed Classification Report (precision, recall, f1-score), and a plot of the Loss Curve history.ðŸ’» Usage and WorkflowThe script executes a complete deep learning workflow:Setup and Seed: Imports necessary libraries (PyTorch, torchvision, sklearn, numpy) and sets random seeds for reproducibility.Model Definition: Defines the BasicBlock with a skip connection and the full ResNet class, configured to instantiate a ResNet18 variant with 8 output classes.Data Loading: Downloads CIFAR-10 data.Data Filtering: The custom FilteredDataset class selects only images belonging to the first eight classes and updates their labels to be contiguous (0 to 7).Train/Validation Split: The filtered training data is stratified and split into 80% for training (using augmented data) and 20% for validation (using non-augmented data).DataLoader Setup: Creates DataLoader objects for efficient batch processing on training, validation, and test sets.Training Initialization: Sets the loss function (CrossEntropyLoss) and initializes the optimized SGD optimizer and the Cosine Annealing LR Scheduler.Training Loop: Iterates for 100 epochs, performing:Forward pass, loss calculation, and backpropagation on the training data.Stepping the LR scheduler after each epoch.Evaluating loss on the validation set (model.eval() with torch.no_grad()).Final Evaluation: After training, the model's performance is measured on the dedicated test set, reporting overall accuracy and class-specific metrics via the Classification Report.Visualization: Generates a plot of the Training Loss vs. Validation Loss over time.
